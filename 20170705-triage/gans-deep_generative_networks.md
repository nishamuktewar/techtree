GANs fall within the class of Generative Models (see [Open AI summary of Generative Models](https://blog.openai.com/generative-models/)), other prominent members of that class are Variational Autoencoders (VAEs) and autoregressive models. The basic idea is the following, since neural networks are universal function approximators, we can use neural networks to learn how to generate samples (i.e., generative). "Generator networks are essentially just parametrized, computational procedures for generating samples, where the architecture provides the family of possible distributions to sample from and the parameters select a distribution from within that family," according to Bengio, Goodfellow, and Couville (Deel Learning, p. 684). 

GANs are one (elegant) approach to training the generative network through game-theory style adversarial training (i.e., no need for MCMC or approximation of posterior). GANs consist of two networks, a discriminator network to classify input as real (real data point) or fake (generated by the other network, the generator). While there are different variants of their loss functions, the basic idea is that the discriminator tries to do accurate classification while the generator tries to fool the discriminator. When they reach an equilibrium (i.e., converge), the generative model is trained (the discriminator can be discarded). Oh also, we only need unlabelled data (unsupervised algorithm). 

The main challenge with GANs is that it is hard to get them to converge. Goodfellow (2014) developed an improved loss function that, in theory, should give better convergence, but it doesn't; little understood heuristics achieve best performance to date (see list of [GAN hacks](https://github.com/soumith/ganhacks)). This is an active area of [theoretical](https://arxiv.org/abs/1701.04862) and applied research. Compared to VAE, GANs provide outputs, e.g., images, of better quality (VAE results tend to be blurry). 

For introductions to GANs, have a look at [Jon Bruner's O'Reilly tutorial (with code)](https://www.oreilly.com/learning/generative-adversarial-networks-for-beginners) ([alternative with code](http://blog.aylien.com/introduction-generative-adversarial-networks-code-tensorflow/)), the [OpenAI post on generative models](https://blog.openai.com/generative-models/), [Ian Goodfellow's podcast interview](https://blogs.nvidia.com/blog/2017/05/17/generative-adversarial-network/), have a look at the original [Goodfellow 2014 paper](https://arxiv.org/pdf/1406.2661.pdf), [Goodfellow's NIPS 2016 tutorial](https://arxiv.org/abs/1701.00160). For more, have a look at paper aggregators for GANs [here](https://github.com/zhangqianhui/AdversarialNetsPapers) and [here](https://github.com/nightrome/really-awesome-gan). 

There are many interesting applications for GANs and generative models in general: rotating faces, changing body posture, image denoising, inpaining (i.e., reconstruction of corrupted image parts), super-resolution, and also pre-training where labelled data is expensive (i.e., representation learning from unlabelled images). The real promise of GANs is of course representation / manifold learning, i.e., the discovery of lower dimensional representation of our world. 

*Notable papers / research breakthroughs:*
PixelRNN - image generation - https://arxiv.org/abs/1601.06759
DRAW - image generation - https://arxiv.org/abs/1502.04623 (VAE based)
Attend, Infer, Repeat - scene understanding - https://arxiv.org/abs/1603.08575
InfoGAN - extension of GANs to maximize disentangled feature learning - https://arxiv.org/abs/1606.03657 (super interesting)
GANs with Laplacian Pyramid - better results on image generation than GANs - https://arxiv.org/abs/1506.05751
DCGAN - GANs with Convolutions (inverting the pooling operation requires assumptions) - https://arxiv.org/abs/1511.06434

It would be easy to get good data (since we don't need labels) and there are many applications. As a report topic, worry is that the field is still too immature for commercialized solution. My verdics, take more time to explore the topic to assess maturity and prototype ideas. 

Also https://medium.com/@ageitgey/abusing-generative-adversarial-networks-to-make-8-bit-pixel-art-e45d9b96cee7 and 

https://arxiv.org/abs/1610.06918 neural networks can learn encryption. Coverage in the blog: https://blog.acolyer.org/2017/02/10/learning-to-protect-communications-with-adversarial-neural-cryptography/ adversarial training to allow to nets (with with access to secret key) to learn how to secretly communicate with one another while a third net (w/o access to key) fails. Input texts are condensed and encrypted by passing them through a series of convolutions, parameters are learning in adversarial fashion (cooperative, really, between "Alice" and "Bob"). 